{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laraschwarz/NYTConnectionsAI/blob/main/WordConnections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4YzLMwYm039"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHIdS1F9sjPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in dataloader:\n",
        "        inputs, labels = batch\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "AA8Xlo1Rsh-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "bert = \"bert-large-uncased\"  # Specify the BERT model to use\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert)  # Load tokenizer for text processing\n",
        "model = AutoModel.from_pretrained(bert, output_hidden_states=True)  # Load BERT model with hidden states\n",
        "\n",
        "\n",
        "def group_words(words):\n",
        "    \"\"\"\n",
        "    Groups words based on semantic similarity using BERT representations and K-Means clustering.\n",
        "\n",
        "    Args:\n",
        "        words (list): A list of words to group. The length must be divisible by 4.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of lists, where each inner list represents a group of semantically similar words.\n",
        "    \"\"\"\n",
        "    # Check if the length of words is divisible by 4\n",
        "    if len(words) % 4 != 0:\n",
        "        raise ValueError(\"The length of input words must be divisible by 4.\")\n",
        "\n",
        "    # Tokenize the words using the BERT tokenizer\n",
        "    sequences = tokenizer(words, padding=True, truncation=True, return_tensors=\"pt\")  # Tokenize, pad, and convert to PyTorch tensors\n",
        "\n",
        "    # Get BERT embeddings without gradient calculations\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**sequences)  # Pass sequences to BERT model\n",
        "        hidden_states = outputs.hidden_states[-1]  # Extract the last layer hidden states\n",
        "\n",
        "    # Reshape hidden states for similarity calculation\n",
        "    hidden_states = hidden_states.view(hidden_states.size(0), -1)  # Flatten for cosine similarity\n",
        "\n",
        "    # Calculate pairwise cosine similarities between word embeddings\n",
        "    similarities = cosine_similarity(hidden_states, hidden_states)\n",
        "\n",
        "    # Perform K-Means clustering to group similar words\n",
        "    clustering = KMeans(n_clusters=len(words)//4, n_init=100)  # Create clusters based on input length\n",
        "    labels = clustering.fit_predict(similarities)  # Assign each word to a cluster\n",
        "\n",
        "    # Organize words into their respective clusters\n",
        "    groups = [[] for _ in range(len(words)//4)]  # Create empty lists for clusters\n",
        "    for i, word in enumerate(words):\n",
        "        group_index = labels[i]\n",
        "        if len(groups[group_index]) < 4:\n",
        "            groups[group_index].append(word)  # Add words to their assigned clusters\n",
        "        else:\n",
        "            # If a group already has 4 words, find the next available group\n",
        "            for j in range(len(groups)):\n",
        "                if len(groups[j]) < 4:\n",
        "                    groups[j].append(word)\n",
        "                    break\n",
        "\n",
        "    return groups  # Return the list of grouped words"
      ],
      "metadata": {
        "id": "6wPOH3cFttGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of words\n",
        "words = [\"red\", \"green\", \"purple\", \"blue\", \"apple\", \"banana\", \"grape\", \"watermelon\", \"basketball\", \"football\", \"soccer\", \"baseball\", \"computer\", \"keyboard\", \"mouse\", \"monitor\"]\n",
        "words2 = [\"tesla\", \"UCSD\", \"toyota\", \"ferrari\", \"SDSU\", \"honda\", \"USC\", \"USD\", \"burger\", \"pizza\", \"pasta\", \"wings\"]\n",
        "words3 = [\"happy\", \"sad\", \"angry\", \"bored\", \"dancing\", \"running\", \"jumping\", \"lifting\"]\n",
        "words4 = [\"forest\", \"parade\", \"erotic\", \"skeleton\", \"train\", \"hedgehog\", \"olive\", \"book\", \"mint\", \"democratic\", \"cactus\", \"fleet\", \"noble\", \"caravan\", \"sad\", \"lime\"]\n",
        "\n",
        "# Call the group_words function to group the words based on semantic similarity\n",
        "groups = group_words(words)\n",
        "groups2 = group_words(words2)\n",
        "groups3 = group_words(words3)\n",
        "groups4 = group_words(words4)\n",
        "\n",
        "print(\"Test #1:\\n\")\n",
        "\n",
        "# Print each group of words\n",
        "for i, group in enumerate(groups):\n",
        "    # Add 1 to the index for more natural group numbering\n",
        "    print(f\"Group {i+1}: {', '.join(group)}\")\n",
        "\n",
        "print(\"\\nTest #2:\\n\")\n",
        "\n",
        "for i, group in enumerate(groups2):\n",
        "    # Add 1 to the index for more natural group numbering\n",
        "    print(f\"Group {i+1}: {', '.join(group)}\")\n",
        "\n",
        "print(\"\\nTest #3:\\n\")\n",
        "\n",
        "for i, group in enumerate(groups3):\n",
        "    # Add 1 to the index for more natural group numbering\n",
        "    print(f\"Group {i+1}: {', '.join(group)}\")\n",
        "\n",
        "print(\"\\nTest #4:\\n\")\n",
        "\n",
        "for i, group in enumerate(groups4):\n",
        "    # Add 1 to the index for more natural group numbering\n",
        "    print(f\"Group {i+1}: {', '.join(group)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LapHZzLmuEjN",
        "outputId": "38788453-c3f7-4fa9-f4e3-8f98afccc3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test #1:\n",
            "\n",
            "Group 1: apple, banana, grape, computer\n",
            "Group 2: basketball, football, soccer, baseball\n",
            "Group 3: watermelon, keyboard, mouse, monitor\n",
            "Group 4: red, green, purple, blue\n",
            "\n",
            "Test #2:\n",
            "\n",
            "Group 1: tesla, toyota, ferrari, honda\n",
            "Group 2: UCSD, SDSU, USC, USD\n",
            "Group 3: burger, pizza, pasta, wings\n",
            "\n",
            "Test #3:\n",
            "\n",
            "Group 1: sad, angry, bored, jumping\n",
            "Group 2: happy, dancing, running, lifting\n",
            "\n",
            "Test #4:\n",
            "\n",
            "Group 1: olive, democratic, fleet, noble\n",
            "Group 2: forest, parade, skeleton, book\n",
            "Group 3: hedgehog, caravan, sad, lime\n",
            "Group 4: erotic, train, mint, cactus\n"
          ]
        }
      ]
    }
  ]
}